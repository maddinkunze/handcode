[[07/24/2023 05:04:24 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:04:27 PM]] all parameters:
[[07/24/2023 05:04:27 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:04:27 PM]] trainable parameters:
[[07/24/2023 05:04:27 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:04:27 PM]] trainable parameter count:
[[07/24/2023 05:04:27 PM]] 3632431
[[07/24/2023 05:04:27 PM]] built graph
[[07/24/2023 05:04:27 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:04:27 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:04:46 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:04:50 PM]] all parameters:
[[07/24/2023 05:04:50 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:04:50 PM]] trainable parameters:
[[07/24/2023 05:04:50 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:04:50 PM]] trainable parameter count:
[[07/24/2023 05:04:50 PM]] 3632431
[[07/24/2023 05:04:50 PM]] built graph
[[07/24/2023 05:04:50 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:04:50 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:36:25 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:36:28 PM]] all parameters:
[[07/24/2023 05:36:28 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:36:28 PM]] trainable parameters:
[[07/24/2023 05:36:28 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:36:28 PM]] trainable parameter count:
[[07/24/2023 05:36:28 PM]] 3632431
[[07/24/2023 05:36:28 PM]] built graph
[[07/24/2023 05:36:28 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:36:28 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:36:47 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:36:51 PM]] all parameters:
[[07/24/2023 05:36:51 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:36:51 PM]] trainable parameters:
[[07/24/2023 05:36:51 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:36:51 PM]] trainable parameter count:
[[07/24/2023 05:36:51 PM]] 3632431
[[07/24/2023 05:36:51 PM]] built graph
[[07/24/2023 05:36:51 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:36:51 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:39:57 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:40:01 PM]] all parameters:
[[07/24/2023 05:40:01 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:40:01 PM]] trainable parameters:
[[07/24/2023 05:40:01 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:40:01 PM]] trainable parameter count:
[[07/24/2023 05:40:01 PM]] 3632431
[[07/24/2023 05:40:01 PM]] built graph
[[07/24/2023 05:40:01 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:40:01 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:40:20 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:40:25 PM]] all parameters:
[[07/24/2023 05:40:25 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:40:25 PM]] trainable parameters:
[[07/24/2023 05:40:25 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:40:25 PM]] trainable parameter count:
[[07/24/2023 05:40:25 PM]] 3632431
[[07/24/2023 05:40:25 PM]] built graph
[[07/24/2023 05:40:25 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:40:25 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:46:19 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:46:22 PM]] all parameters:
[[07/24/2023 05:46:22 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:46:22 PM]] trainable parameters:
[[07/24/2023 05:46:22 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:46:22 PM]] trainable parameter count:
[[07/24/2023 05:46:22 PM]] 3632431
[[07/24/2023 05:46:22 PM]] built graph
[[07/24/2023 05:46:22 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:46:22 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:46:41 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 05:46:45 PM]] all parameters:
[[07/24/2023 05:46:45 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 05:46:45 PM]] trainable parameters:
[[07/24/2023 05:46:45 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 05:46:45 PM]] trainable parameter count:
[[07/24/2023 05:46:45 PM]] 3632431
[[07/24/2023 05:46:45 PM]] built graph
[[07/24/2023 05:46:45 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 05:46:45 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 06:02:04 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 06:02:07 PM]] all parameters:
[[07/24/2023 06:02:08 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 06:02:08 PM]] trainable parameters:
[[07/24/2023 06:02:08 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 06:02:08 PM]] trainable parameter count:
[[07/24/2023 06:02:08 PM]] 3632431
[[07/24/2023 06:02:08 PM]] built graph
[[07/24/2023 06:02:08 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 06:02:08 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 06:02:22 PM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/24/2023 06:02:30 PM]] all parameters:
[[07/24/2023 06:02:30 PM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/24/2023 06:02:31 PM]] trainable parameters:
[[07/24/2023 06:02:31 PM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/24/2023 06:02:31 PM]] trainable parameter count:
[[07/24/2023 06:02:31 PM]] 3632431
[[07/24/2023 06:02:31 PM]] built graph
[[07/24/2023 06:02:31 PM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/24/2023 06:02:31 PM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:15:47 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:15:51 AM]] all parameters:
[[07/25/2023 12:15:52 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:15:52 AM]] trainable parameters:
[[07/25/2023 12:15:52 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:15:52 AM]] trainable parameter count:
[[07/25/2023 12:15:52 AM]] 3632431
[[07/25/2023 12:15:52 AM]] built graph
[[07/25/2023 12:15:53 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:15:53 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:16:28 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:16:32 AM]] all parameters:
[[07/25/2023 12:16:32 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:16:32 AM]] trainable parameters:
[[07/25/2023 12:16:32 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:16:32 AM]] trainable parameter count:
[[07/25/2023 12:16:33 AM]] 3632431
[[07/25/2023 12:16:33 AM]] built graph
[[07/25/2023 12:16:33 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:16:33 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:16:57 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:17:01 AM]] all parameters:
[[07/25/2023 12:17:01 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:17:02 AM]] trainable parameters:
[[07/25/2023 12:17:02 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:17:02 AM]] trainable parameter count:
[[07/25/2023 12:17:02 AM]] 3632431
[[07/25/2023 12:17:02 AM]] built graph
[[07/25/2023 12:17:03 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:17:03 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:18:25 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:18:29 AM]] all parameters:
[[07/25/2023 12:18:30 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:18:30 AM]] trainable parameters:
[[07/25/2023 12:18:30 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:18:31 AM]] trainable parameter count:
[[07/25/2023 12:18:31 AM]] 3632431
[[07/25/2023 12:18:31 AM]] built graph
[[07/25/2023 12:18:31 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:18:31 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:19:00 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:19:04 AM]] all parameters:
[[07/25/2023 12:19:04 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:19:05 AM]] trainable parameters:
[[07/25/2023 12:19:05 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:19:05 AM]] trainable parameter count:
[[07/25/2023 12:19:05 AM]] 3632431
[[07/25/2023 12:19:05 AM]] built graph
[[07/25/2023 12:19:05 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:19:06 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:19:28 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:19:33 AM]] all parameters:
[[07/25/2023 12:19:33 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:19:33 AM]] trainable parameters:
[[07/25/2023 12:19:33 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:19:34 AM]] trainable parameter count:
[[07/25/2023 12:19:34 AM]] 3632431
[[07/25/2023 12:19:34 AM]] built graph
[[07/25/2023 12:19:34 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:19:34 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:19:59 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:20:04 AM]] all parameters:
[[07/25/2023 12:20:04 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:20:05 AM]] trainable parameters:
[[07/25/2023 12:20:05 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:20:05 AM]] trainable parameter count:
[[07/25/2023 12:20:05 AM]] 3632431
[[07/25/2023 12:20:05 AM]] built graph
[[07/25/2023 12:20:05 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:20:06 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:44:38 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:44:41 AM]] all parameters:
[[07/25/2023 12:44:41 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:44:42 AM]] trainable parameters:
[[07/25/2023 12:44:42 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:44:42 AM]] trainable parameter count:
[[07/25/2023 12:44:42 AM]] 3632431
[[07/25/2023 12:44:42 AM]] built graph
[[07/25/2023 12:44:42 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:44:42 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:44:47 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:44:51 AM]] all parameters:
[[07/25/2023 12:44:51 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:44:51 AM]] trainable parameters:
[[07/25/2023 12:44:51 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:44:52 AM]] trainable parameter count:
[[07/25/2023 12:44:52 AM]] 3632431
[[07/25/2023 12:44:52 AM]] built graph
[[07/25/2023 12:44:52 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:44:52 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:48:06 AM]] 
new run with parameters:
{'attention_mixture_components': 10,
 'batch_size': 32,
 'batch_sizes': [32, 64, 64],
 'beta1_decay': 0.9,
 'beta1_decays': [0.9, 0.9, 0.9],
 'checkpoint_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\checkpoints',
 'early_stopping_steps': 1500,
 'enable_parameter_averaging': False,
 'grad_clip': 10,
 'keep_prob_scalar': 1.0,
 'learning_rate': 0.0001,
 'learning_rates': [0.0001, 5e-05, 2e-05],
 'log_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\logs',
 'log_interval': 20,
 'logging_level': 20,
 'loss_averaging_window': 100,
 'lstm_size': 400,
 'min_steps_to_checkpoint': 2000,
 'num_restarts': 2,
 'num_training_steps': 100000,
 'optimizer': 'rms',
 'output_mixture_components': 20,
 'output_units': 121,
 'patiences': [1500, 1000, 500],
 'prediction_dir': 'D:\\Projekte\\HandCode\\src\\lib\\handwriting\\predictions',
 'reader': None,
 'regularization_constant': 0.0,
 'restart_idx': 0,
 'validation_batch_size': 32,
 'warm_start_init_step': 17900}
[[07/25/2023 12:48:09 AM]] all parameters:
[[07/25/2023 12:48:09 AM]] [('Variable:0', []),
 ('Variable_1:0', []),
 ('Variable_2:0', []),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/kernel/RMSProp_1:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/weights/RMSProp_1:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp:0', [30]),
 ('rnn/LSTMAttentionCell/attention/biases/RMSProp_1:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias/RMSProp_1:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel/RMSProp_1:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias/RMSProp_1:0', [1600]),
 ('rnn/gmm/weights/RMSProp:0', [400, 121]),
 ('rnn/gmm/weights/RMSProp_1:0', [400, 121]),
 ('rnn/gmm/biases/RMSProp:0', [121]),
 ('rnn/gmm/biases/RMSProp_1:0', [121])]
[[07/25/2023 12:48:10 AM]] trainable parameters:
[[07/25/2023 12:48:10 AM]] [('rnn/LSTMAttentionCell/lstm_cell/kernel:0', [476, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/attention/weights:0', [476, 30]),
 ('rnn/LSTMAttentionCell/attention/biases:0', [30]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_1/bias:0', [1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/kernel:0', [876, 1600]),
 ('rnn/LSTMAttentionCell/lstm_cell_2/bias:0', [1600]),
 ('rnn/gmm/weights:0', [400, 121]),
 ('rnn/gmm/biases:0', [121])]
[[07/25/2023 12:48:10 AM]] trainable parameter count:
[[07/25/2023 12:48:10 AM]] 3632431
[[07/25/2023 12:48:10 AM]] built graph
[[07/25/2023 12:48:10 AM]] restoring model parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
[[07/25/2023 12:48:10 AM]] Restoring parameters from D:\Projekte\HandCode\src\lib\handwriting\checkpoints\model-17900
